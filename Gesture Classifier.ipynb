{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ee193c",
   "metadata": {},
   "source": [
    "# WELCOME, PLEASE ONLY RUN THE PREDICTION CELL AS THE MODEL HAS ALREADY BEEN TRAINED WITH MY DATASET."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c0859",
   "metadata": {},
   "source": [
    "## Model Cell. CNN is used with pooling and the generator structure given in the labs, to train via a series of 64x64 RGB images for real time gesture prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c75bd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import cv2\n",
    "import sys, os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Variable definitions:\n",
    "image_height = 64    # Height of input images\n",
    "image_width = 64     # Width of input images.\n",
    "dimensions = 3       # Number of image channels (R G B = 3 -> 255 x 3).\n",
    "batch_size = 5       # Batch size for model.\n",
    "gestures = 7         # Number of gestures (model outputs).\n",
    "\n",
    "# Build the Sequential Convolutional Neural Network to classify our gestures.\n",
    "Model = Sequential()\n",
    "\n",
    "# Layer 1 - 32 nodes, first pooling layer.\n",
    "Model.add(Convolution2D(32, (3, 3), input_shape = (image_height, image_width, dimensions), activation = 'relu'))\n",
    "Model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Layer 2 - 32 nodes, second pooling layer.\n",
    "Model.add(Convolution2D(32, (3, 3), activation = 'relu'))\n",
    "Model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flatten the layers.\n",
    "Model.add(Flatten())\n",
    "\n",
    "# Fully Connected Layer / output layer (7 gestures).\n",
    "Model.add(Dense(units = 128, activation = 'relu'))\n",
    "Model.add(Dense(units = gestures, activation = 'softmax'))\n",
    "\n",
    "# Optimizer definition.\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# Compile the CNN Model.\n",
    "Model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "\n",
    "# Initialise the generators to train the model with the train/test directories.\n",
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# train / test directories, resize for the 64 x 64 input for model.\n",
    "train_generator = train_datagen.flow_from_directory('images/train', target_size = (image_height, image_width),\n",
    "                                                 batch_size = batch_size, color_mode = 'rgb', class_mode = 'categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory('images/test', target_size = (image_height, image_width),\n",
    "                                            batch_size = batch_size, color_mode = 'rgb', class_mode = 'categorical')\n",
    "\n",
    "# Allocate steps based on the amount of data and the batch size we are using:\n",
    "steps_per_epoch = len(train_generator) / batch_size + 1\n",
    "validation_steps = len(test_generator) / batch_size + 1\n",
    "\n",
    "# Fit and train the model.\n",
    "Model.fit_generator(train_generator, steps_per_epoch = steps_per_epoch, epochs = 200, validation_data = test_generator,\n",
    "                         validation_steps = validation_steps)\n",
    "\n",
    "\n",
    "# Save the model, so that Li does not have to train it.\n",
    "model_json = Model.to_json()\n",
    "with open(\"trained_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the optimal weights for the model.\n",
    "Model.save_weights('optimal_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b152a5",
   "metadata": {},
   "source": [
    "# RUN CELL BELOW:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9d043",
   "metadata": {},
   "source": [
    "## Prediction Cell. The gesture is captured via a smaller region of interest within the camera, so as to eliminate some noise, then scaled down and fed to the model. The most probable gesture is displayed at the top left in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import cv2\n",
    "import sys, os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Variable redefinitions, for single run use-case.\n",
    "image_height = 64\n",
    "image_width = 64\n",
    "dimensions = 3\n",
    "\n",
    "# Loading the model\n",
    "json_file = open(\"trained_model.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "Model = model_from_json(model_json)\n",
    "\n",
    "# load the trained optimal weights into the model.\n",
    "Model.load_weights(\"optimal_model_weights.h5\")\n",
    "\n",
    "# Start the camera.\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Main loop:\n",
    "while True:\n",
    "    # Initialisation.\n",
    "    _, frame = video.read()\n",
    "    \n",
    "    # Flip the image in order to display it normally, not inverted.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Coordinates of the ROI - Region of interest that the gesture will be extracted from.\n",
    "    x1 = int(0.5 * frame.shape[1])\n",
    "    y1 = 10\n",
    "    x2 = frame.shape[1] - 10\n",
    "    y2 = int(0.5 * frame.shape[1])\n",
    "    \n",
    "    # Draw the ROI to the frame, inc/decrement by 1 due to the bounding box.\n",
    "    cv2.rectangle(frame, (x1 - 1, y1 - 1), (x2 + 1, y2 + 1), (255, 0, 0) , 1)\n",
    "    \n",
    "    # Extract, resize and then show the ROI seperately.\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    roi = cv2.resize(roi, (image_height, image_width)) \n",
    "    cv2.imshow(\"ROI Frame\", roi)\n",
    "    \n",
    "    # Use our convolutional neural network to make a gesture prediction in real time:\n",
    "    result = Model.predict(roi.reshape(1, image_height, image_width, dimensions))\n",
    "    \n",
    "    # Dictionary of each gesture, mapped to the models corresponding result - 2D array. \n",
    "    gesture = {'ZERO': result[0][0], \n",
    "                  'ONE': result[0][1], \n",
    "                  'TWO': result[0][2],\n",
    "                  'THREE': result[0][3],\n",
    "                  'FOUR': result[0][4],\n",
    "                  'DOWN': result[0][5],\n",
    "                  'UP': result[0][6]}\n",
    "    \n",
    "    # Sort, based upon the top prediction (models most likely gesture) - operator.itemgetter grabs the first item (1).\n",
    "    gesture = sorted(gesture.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    # Display the prediction to the camera window in real time.\n",
    "    cv2.putText(frame, gesture[0][0], (10, 120), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)    \n",
    "    cv2.imshow(\"Gesture Recognition Program\", frame)\n",
    "    \n",
    "    # End the program on pressing q.\n",
    "    interrupt = cv2.waitKey(1)\n",
    "    if interrupt == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Kill the windows and the process.\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
